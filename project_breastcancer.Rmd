---
title: "Project: Breast Cancer Diagnosis"
author: "Shengzhi Luo"
date: "3/31/2022"
header-includes:
   - \usepackage{amsmath}
output: pdf_document
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(caret)
library(ggcorrplot)
library(MASS)
library(pROC)
library(glmnet)
```


```{r}
ggplot2::theme_set(theme_minimal() + theme(legend.position = "bottom"))
```


## data import and data clean
```{r, dpi = 300}
#load the data
breast = read.csv("breast-cancer.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::select(-1, -33) %>% #drop id and NA columns
  mutate(diagnosis = recode(diagnosis, "M" = 1, "B" = 0))
#check collinearity
corr = breast[2:31] %>% 
  cor()
ggcorrplot(corr, type = "upper", tl.cex = 8)
#remove some highly correlated variables
breast_dat <- breast %>% dplyr::select(-area_se, -perimeter_se, -area_worst, -perimeter_mean, -perimeter_worst, -area_mean, -radius_worst, -concave_points_mean, -texture_worst, -compactness_mean, -concavity_worst)

corr1 = breast_dat[2:20] %>% 
  cor()
ggcorrplot(corr1, type = "upper", tl.cex = 8)

#partition data into training and test data
set.seed(2022)
trainRows <- createDataPartition(y = breast_dat$diagnosis, p = 0.8, list = FALSE)
breast_train <- breast_dat[trainRows, ]
breast_test <-  breast_dat[-trainRows, ]

head(breast_dat, 5)
r = dim(breast_dat)[1] #row number
c = dim(breast_dat)[2] #column number
var_names = names(breast_dat)[-c(1,2)] #variable names
  
standardize = function(col) {
  mean = mean(col)
  sd = sd(col)
  return((col - mean)/sd)
}
stand_df = breast_dat %>% 
  dplyr::select(radius_mean:fractal_dimension_worst) %>% 
  map_df(.x = ., standardize) #standardize
X = stand_df #predictors
y = breast_dat[,1]#response
```

```{r}
x_train <- breast_train[2:20] #predictors
y_train <- breast_train[1] #response
x_train_stan <- cbind(rep(1, nrow(x_train)), scale(x_train))

x_test <- breast_test[2:20]
x_test_stan <- cbind(rep(1, nrow(x_test)), scale(x_test))
```

# feature plot

```{r, dpi = 300}
data = cbind(y,X)

featurePlot(x = data[, 2:7],
            y = factor(data$y),
            plot = "pairs",
            auto.key = list(columns = 2)
)

featurePlot(x = data[, 8:15],
            y = factor(data$y),
            plot = "pairs",
            auto.key = list(columns = 2)
)

featurePlot(x = data[, 16:20],
            y = factor(data$y),
            plot = "pairs",
            auto.key = list(columns = 2)
)
```

```{r}
mean_data = breast_dat %>% 
  group_by(diagnosis) %>% 
  summarise(across(radius_mean: fractal_dimension_worst, ~ mean(.x, na.rm = TRUE)))
mean_data
```

## Full logistic model
```{r}
glm.fit <- glm(diagnosis ~ ., 
               data = breast_train, 
               family = binomial)

summary(glm.fit)$coefficients %>% knitr::kable()

glm.fit %>% predict(breast_test, type = "response")
pred <- predict(glm.fit, breast_test, type = "response")
y_test <- factor(breast_test$diagnosis)
auc_full <- auc(y_test, pred)
auc_full
```

## Newton-Raphson algorithm
```{r loglikelyhood}
# Write a function that generate log-likelihood, gradient and Hessian 
# Inputs:

# x - data variables 
# y - outcome 
# par - vector of beta parameters 
func = function(x, y, par) {

# Log link x*beta 
  u = x %*% par 
  expu = exp(u)

loglik = vector(mode = "numeric", length(y)) 
for(i in 1:length(y)) 
  loglik[i] = y[i]*u[i] - log(1 + expu[i]) 
loglik_value = sum(loglik)

# Log-likelihood at betavec
p <- 1 / (1 + exp(-u))

# P(Y_i=1|x_i) 
grad = vector(mode = "numeric", length(par))

#grad[1] = sum(y - p) 
for(i in 1:length(par)) 
  grad[i] = sum(t(x[,i])%*%(y - p))

#Hess <- -t(x)%*%p%*%t(1-p)%*%x 
Hess = hess_cal(x, p) 
return(list(loglik = loglik_value, grad = grad, Hess = Hess))

}

# Function to return the Hessian matrix 
hess_cal = function(x,p){

len = length(p) 
hess = matrix(0, ncol(x), ncol(x)) 
for (i in 1:len) {

x_t = t(x[i,])

unit = t(x_t)%*%x_t*p[i]*(1-p[i])

#unit = t(x[i,])%*%x[i,]*p[i]*(1-p[i])

hess = hess + unit 
} 
return(-hess)

}
```

## 2. Newton-Raphson algorithm

input:
x: predictors without intercept
y: response variables
beta: if not specified, 0 will be set to all coefficients
tol: the threshold to end up the function if the difference between loglike function at 2 adjacent steps below this value.
lambda_init: the initial lambda to control the number of each step and lambda will change in halving process.
decay_rate: the ratio of decayed lambda to lambda at last step in havling process.

output:
beta: a vector of coeffients3

```{r}
newton_optimize = function(x, y, beta = NULL, tol = 0.00001, lambda_init = 1, decay_rate = 0.5){
  
  # add the intercept
  x = cbind(rep(1, nrow(x)), x)
  
  # if beta is not specified, set all initial coefficients to 0
  if (is.null(beta))
    beta = matrix(rep(0, ncol(x)))
  
  # calculate the initial gradient, Hessian matrix and negative loglike funtion
  optimization = func(x, y, beta)
  step = 1
  previous_loglik = -optimization$loglik

  # start the interations to optimize the beta
  while (TRUE) {
    print(paste("step:", step, "  negative loglike loss:", -optimization$loglik))
   
    # set initial lambda at this step equals to the parameters, this variable will change in havling step
    lambda = lambda_init
    
    # since there maybe some issues when calculate new beta, so we use try-catch sentence. If some errors ocurr, the beta will be kept as the beta at last step.
    beta_new <- tryCatch({
        beta - lambda * inv(optimization$Hess) %*% optimization$grad # calculate new beta, if no errors, the result will be given to variable "beta_new" 
      }, error = function(err) {return(beta)})

    
    # calculate gradient, Hessian and loglike   
    optimization = func(x, y, beta_new)
   
    
    # havling steps start only when it optimizes at opposite direction.
    # if it optimizes at opposite direction, lambda will be havled to make the step smaller. 
    while (previous_loglik <= -optimization$loglik) {
      lambda = lambda * decay_rate # lambda decay
      
      # same reason to use try-catch
      # but if errors occur, although beta keeps, the lambda will be havled at next step, makes the result different.
      beta_new <- tryCatch({
        beta - lambda * inv(optimization$Hess) %*% optimization$grad
      }, error = function(err) {return(beta)})
      
      # optimize by decayed lambda
      optimization = func(x, y, beta_new)
      
      # if the optimized differences are too small, end up the function and return beta. 
      if ((previous_loglik - -optimization$loglik) <= tol)
        return(beta)
    }
    
    # if the differences calculated from normal calculation or havling steps are too small, end up the function and return beta. 
    if (abs(previous_loglik - -optimization$loglik) <= tol)
      return(beta)
    
    # save the negative loglike value at this step and will be used as previous loglike value at next step.
    previous_loglik = -optimization$loglik
    
    # if the function is not ended up, then the new beta is valid. save it.
    beta = beta_new 
    
    step = step + 1
  }
  
  # so the loop will be ended up by 2 conditions.
  # 1. the differences calculated by havling steps are too small.
  # 2. the differences calculated by normal optimization are too small.
  return(beta)
}


```


```{r}
breast_dat = read.csv("./breast-cancer.csv") %>% 
  janitor::clean_names() %>% 
  dplyr::select(-1, -33) %>% #drop id and NA columns
  mutate(diagnosis = recode(diagnosis, "M" = 1, "B" = 0))

breast_dat <- breast_dat %>% dplyr::select(-area_se, -perimeter_se, -area_worst, -perimeter_mean, -perimeter_worst, -area_mean, -radius_worst, -concave_points_mean, -texture_worst, -compactness_mean, -concavity_worst)
trainRows <- createDataPartition(y = breast_dat$diagnosis, p = 0.8, list = FALSE)
x = breast_dat %>% dplyr::select(-diagnosis) %>% as.matrix()

# make the response variables
y = breast_dat %>% 
  dplyr::select(diagnosis) %>% 
  as.matrix()
glm.fit <- glm(diagnosis ~ ., 
               data = breast_dat, 
               subset = trainRows, 
               family = binomial(link = "logit"))
```
# Loading the data and run function
```{r,warning=FALSE}
x = breast_dat %>% dplyr::select(-diagnosis) %>% as.matrix()

# make the response variables
y = breast_dat %>% 
  dplyr::select(diagnosis) %>% 
  as.matrix()


# calculate beta_hat by newton method 3
beta = newton_optimize(x, y, tol = 0.01)

#coefficients of full and lasso models
newton_raphson_beta <- beta %>% as.vector()
coefnames <- rownames(coef(summary(glm.fit)))
cbind(coefnames, newton_raphson_beta) %>% knitr::kable()

```
                          

## coordinate-wise optimization of a logistic-lasso model

```{r}
x_train <- breast_train[2:20] #predictors
y_train <- breast_train[1] #response
x_train_stan <- cbind(rep(1, nrow(x_train)), scale(x_train))
x_test <- breast_test[2:20]
y_test <- breast_test[1]
```


```{r}
#soft threshold
sfxn <- function(beta, lambda) {
  if (abs(beta) > lambda) {
    return(sign(beta) * (abs(beta) - lambda))
  }
  else {
    return(0)
  }
}
```

```{r}
#coordinate-wise optimization function
coordwise_lasso <- function(lambda, x, y, betastart, tol = exp(-10), maxiter = 5000) {
  i <- 0
  n <- length(y)
  pnum <- length(betastart)
  betavec <- betastart
  loglik <- 0
  res <- c(0, loglik, betavec)
  prevloglik <- -Inf
  while (i < maxiter & abs(loglik - prevloglik) > tol & loglik < Inf) {
    i <- i + 1
    prevloglik <- loglik
    for (j in 1:pnum) {
      theta <- x %*% betavec
      p <- exp(theta) / (1 + exp(theta)) #probability of malignant cases
      w <- p*(1-p) #working weights
      w <- ifelse(abs(w-0) < 1e-5, 1e-5, w)
      z <- theta + (y - p)/w #working response
      zwoj <- x[, -j] %*% betavec[-j]
      betavec[j] <- sfxn(sum(w*(x[,j])*(z - zwoj)), lambda) / (sum(w*x[,j]*x[,j]))
    }
    theta <- x %*% betavec
    p <- exp(theta) / (1 + exp(theta)) #probability of malignant cases
    w <- p*(1-p) #working weights
    w <- ifelse(abs(w-0) < 1e-10, 1e-10, w)
    z <- theta + (y - p)/w
    loglik <- sum(w*(z - theta)^2) / (2*n) + lambda * sum(abs(betavec))
    res <- rbind(res, c(i, loglik, betavec))
  }
  return(res)
}
#coordwise_res <- coordwise_lasso(lambda = 0.006, x_train_stan, y_train, betastart = rep(0, #20))
#coordwise_res[nrow(coordwise_res), ]
```

We need to calculate lambdamax first to define a sequence of lambda. 
```{r}
x.matrix <- scale(x_train) %>% as.matrix()
y.matrix <- as.matrix(y_train)
lambdamax <- max(abs(t(x.matrix) %*% y.matrix)) #/ nrow(y.matrix)
lambda_seq1 <- exp(seq(log(lambdamax), -5, length = 50))
lambda_seq2 <- exp(seq(log(lambdamax), -5, length = 50))
```


```{r}
#a path of solutions
pathwise <- function(x, y, lambda) {
  n <- length(lambda)
  betastart <- rep(0, 20)
  betas <- NULL
  for (i in 1:n) {
    coordwise_res <- coordwise_lasso(lambda = lambda[i],
                                     x = x,
                                     y = y,
                                     betastart = betastart)
    curbeta <- coordwise_res[nrow(coordwise_res), 3:22]
    betastart <- curbeta
    betas <- rbind(betas, c(curbeta))
  }
  return(data.frame(cbind(lambda, betas)))
}
pathwise_sol <- pathwise(x_train_stan, y_train, lambda_seq2)
round(pathwise_sol, 2) %>% knitr::kable()
```


```{r}
colnames(pathwise_sol) <- c("lambda", rownames(coef(summary(glm.fit))))
pathwise_sol %>% 
  pivot_longer(
    3:21,
    names_to = "variables",
    values_to = "coefficients") %>% 
  ggplot(aes(x = log(lambda), y = coefficients, group = variables, color = variables)) +
  geom_line() + 
  geom_vline(xintercept = log(0.981), linetype = 2) +
  ggtitle("A path of solutions for a descending sequence of lambda") +
  xlab("log(Lambda)") + 
  ylab("Coefficients")
```


## cross-validation
```{r,warning=FALSE}
set.seed(2022)
cv = function(data, lambda) {
  n <- nrow(data)
  data <- data[sample(n), ] #shuffle the data
  folds <- cut(seq(1, nrow(data)), breaks = 5, labels = FALSE) #Create 5 equal size folds
 # mse <- data.frame() #a data frame storing mse results
  #mse_lambda <- vector()
  #se <- vector() #a vector storing test errors
  res <- lambda 
  #se <- vector() #a vectro storing test errors
  
    #Perform 5 fold cross validation
  for (i in 1:5) {
    #partition the data into train and test data
    testRows <- which(folds == i, arr.ind = TRUE)
    data_test <- data[testRows, ]
    data_train <- data[-testRows, ]
    x_train <- data_train[2:20]
    x_train_stan <- cbind(rep(1, nrow(x_train)), scale(x_train))
    y_train <- data_train[1]
    x_test <- data_test[2:20]
    #standardized test data
    x_test_stan <- cbind(rep(1, nrow(x_test)), scale(x_test))
    y_test <- data_test %>% mutate(diagnosis = factor(diagnosis))
    y_test <- y_test$diagnosis
    #Use the test and train data partitions to perform lasso
    path_sol <- pathwise(x = x_train_stan,
                         y = y_train,
                         lambda = lambda)
    auc <- vector()
    for (j in 1:length(lambda)) {
      curbeta <- as.numeric(path_sol[j, 2:21])
      theta <- x_test_stan %*% curbeta
      p <- exp(theta) / (1 + exp(theta)) 
      auc[j] <- auc(y_test, p)
      #y.pred <- ifelse(p > 0.5, 1, 0)
      #accuracy[j] <- mean(y.pred == y_test)
    }
    print(auc)
    res <- cbind(res, auc)
    print(res)
  }
  return(res)
    #se[j] <- sqrt(var(error)/5)
  #cv.auc.lambda <- rowMeans(mse)
  #return(cv.auc.lambda)
}
cv_test = cv(data = breast_train, lambda_seq2)


cv_res <- as.data.frame(cv_test) #colnames(c("auc1", "auc2", "auc3", "auc4", "auc5"))
colnames(cv_res) <- c("res", "auc1", "auc2", "auc3", "auc4", "auc5")
cv_lambda <- cv_res[1]
mean_auc <- cv_res %>% dplyr::select(-1) %>% rowMeans()
cv_auc <- cbind(cv_lambda, mean_auc)
maxauc <- max(cv_auc$mean_auc)
bestlambda <- cv_auc[which(cv_auc$mean_auc == maxauc ),]$res
cv_auc %>% 
  ggplot(x = res, y = mean_auc ) +
  geom_line(aes(x = res, y = mean_auc), col = "blue") +
  geom_vline(xintercept = bestlambda, linetype = "dashed", col = "red") +
  labs(title = "Mean AUC vs. Lambda",
       x = "Lambda",
       y = "Mean AUC")
```


## Compare full model and lasso model
```{r,warning=FALSE}
#corresponding betas of best lambda
lasso_beta <- pathwise_sol[which(pathwise_sol$lambda == bestlambda ),][2:21] %>% as.numeric()

#prediction performance function
predict <- function(x, y, betavec) {
  theta <- x %*% betavec
  p <- exp(theta) / (1 + exp(theta))
  auc <- auc(y, p)
  }
y_test <- factor(breast_test$diagnosis)

auc_lasso <- predict(x_test_stan, y_test, lasso_beta)
auc_lasso


cbind(auc_full, auc_lasso) %>% knitr::kable()

```


```{r}
#coefficients of full and lasso models
glm_beta <- glm.fit$coefficients %>% as.vector()
coefnames <- rownames(coef(summary(glm.fit)))
cbind(coefnames, glm_beta, lasso_beta) %>% knitr::kable()
```

